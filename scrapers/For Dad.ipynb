{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaner(r, u):\n",
    "    try:\n",
    "        c = r[0].p.get_text(strip=True)\n",
    "    except:\n",
    "        c = ''\n",
    "    try:\n",
    "        d = r[3].img['src']\n",
    "    except:\n",
    "        d = ''\n",
    "    return  [u+r[0].a['href'], r[0].a.get_text(strip=True), c, r[1].get_text(strip=True), r[2].get_text(strip=True), d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = ['company-description', 'company-contact', 'company-info']\n",
    "\n",
    "def cleaner_(r):\n",
    "    vals = [l.a['href'] for l in r[1].find_all('li', class_ = 'actions')]\n",
    "    a = [v.split(':')[1] if v[0] in ['c', 'm']  else v for v in vals]\n",
    "    b = r[0].get_text(strip=True)\n",
    "    c = '\\n'.join([d.get_text() for d in r[1].address.find_all('div')][1:])\n",
    "    r2 = r[2].find_all('div', recursive = False)\n",
    "    vals_ = [a_ for a_ in r2[1].find_all('li')]\n",
    "    e = []\n",
    "    for v_ in vals_:\n",
    "        v = unicode(v_.h5.text)\n",
    "        v_.h5.clear()\n",
    "        e.append((v, \" \".join(v_.get_text(strip=True).split())))\n",
    "    try:\n",
    "        r2[2]\n",
    "    except IndexError:\n",
    "        d = []\n",
    "    else:\n",
    "        d = [(a_.get_text(), a_.a['href']) for a_ in r2[2].find_all(class_ = 'file')]\n",
    "    return [b]+a+[c]+e+d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.swedishlifesciences.se/company-index?title=&page=0\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=1\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=2\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=3\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=4\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=5\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=6\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=7\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=8\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=9\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=10\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=11\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=12\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=13\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=14\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=15\n",
      "http://www.swedishlifesciences.se/company-index?title=&page=16\n"
     ]
    }
   ],
   "source": [
    "companies = []\n",
    "for p in range(0, 17):\n",
    "    url = 'http://www.swedishlifesciences.se'\n",
    "    ext = \"/company-index?title=&page=\"+str(int(p))\n",
    "    print url+ext\n",
    "    soup = BeautifulSoup(requests.get(url+ext).text, \"html.parser\")\n",
    "    data = soup.table\n",
    "    companies = companies+[cleaner(rows.find_all('td'), url) for rows in data.find_all('tr', class_ = \"company\")]\n",
    "    time.sleep(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies_full = []\n",
    "print len(companies)\n",
    "for i, c in enumerate(companies):\n",
    "    try:\n",
    "        if not i % 10:\n",
    "            print i\n",
    "        url = c[0]\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "        data = soup.find(class_  = 'main-content')\n",
    "        if len(data.find_all('div')) == 3:\n",
    "            companies_full.append(c+[data.find_all('div')[2].a['href']])\n",
    "        else:\n",
    "            companies_full.append(c+['']+cleaner_(data.find_all(class_ = classes)))\n",
    "        time.sleep(10)\n",
    "    except:\n",
    "        print c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/dad_data.csv'\n",
    "with open(path, 'wb') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['URL', 'NAME', 'MOTTO', 'CITY', 'CATEGORY', 'LOGO_LINK', 'COMPANY_WEBSITE', 'OTHER_DATA'])\n",
    "    w.writerows([unicode(c_).encode(\"utf-8\") for c_ in c] for c in companies_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.biotechumea.se/'\n",
    "ext = 'pharmaceuticals-and-diagnostics'\n",
    "soup = BeautifulSoup(requests.get(url+ext).text, \"html.parser\")\n",
    "data = soup.find_all('div', class_ = 'item-list company-list')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp_list = [[l.text, url+l.a['href']] for l in data.find_all('li')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.biotechumea.se/essum \n",
      "http://www.biotechumea.se/helicure \n",
      "http://www.biotechumea.se/inrobiomedtek \n",
      "http://www.biotechumea.se/it-instrumentteknik \n",
      "http://www.biotechumea.se/pelagia-miljokonsult \n",
      "http://www.biotechumea.se/techtum-lab \n"
     ]
    }
   ],
   "source": [
    "comp_list_full = []\n",
    "for c in comp_list:\n",
    "    try:\n",
    "        soup = BeautifulSoup(requests.get(c[1]).text, \"html.parser\")\n",
    "        data = soup.find('div', class_ = 'CS_article')\n",
    "        a = [d.text for d in data.find_all(['h1', 'div'], recursive=False)[:-1]]+[d.get_text(strip=True) for d in data.find_all('div', recursive=False)[-1].find_all(['p', 'div'], recursive=False)]\n",
    "        comp_list_full = comp_list_full+[c+a]\n",
    "    except:\n",
    "        comp_list_full = comp_list_full+[c]\n",
    "        print c[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/dad_data_2.csv'\n",
    "with open(path, 'wb') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['NAME', 'LINK', 'DESCRIPTION', 'DESCRIPTION2', 'CONTACT', 'CONTACT_DETAILS'])\n",
    "    w.writerows([unicode(c_).encode(\"utf-8\") for c_ in c] for c in comp_list_full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
